<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://achchg.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://achchg.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-11T17:35:50+00:00</updated><id>https://achchg.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Langfuse</title><link href="https://achchg.github.io/blog/2024/Langfuse/" rel="alternate" type="text/html" title="Langfuse"/><published>2024-04-01T00:00:00+00:00</published><updated>2024-04-01T00:00:00+00:00</updated><id>https://achchg.github.io/blog/2024/Langfuse</id><content type="html" xml:base="https://achchg.github.io/blog/2024/Langfuse/"><![CDATA[<p>I also explored Langfuse as an alternative to LangSmith, specifically the local host option, <a href="https://langfuse.com/docs/deployment/local">Self-host &gt; Local (docker compose)</a>.</p> <h4 id="pre-requisite-and-setup">Pre-requisite and setup</h4> <ul> <li>Getting started was easy. However, the default port used in localhost is port <code class="language-plaintext highlighter-rouge">3000</code>, which has been occupied in my case. Therefore, I modified <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> to map toward another local port, e.g. <code class="language-plaintext highlighter-rouge">3030</code>. Then again, <code class="language-plaintext highlighter-rouge">docker compose up</code>.</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ports:
      - "3030:3000"
</code></pre></div></div> <ul> <li>Now, we can access Langfuse at <code class="language-plaintext highlighter-rouge">http://localhost:3030</code> instead.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/langfuse1-480.webp 480w,/assets/img/langfuse1-800.webp 800w,/assets/img/langfuse1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/langfuse1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>You’d see the below screen and can click sign-up to setup a user account for local usage.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/langfuse2-480.webp 480w,/assets/img/langfuse2-800.webp 800w,/assets/img/langfuse2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/langfuse2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>After setting up an account and a project, here are some screenshots over the interface of langfuse and how you can go ahead and setup API keys for local dev usage.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/langfuse4-480.webp 480w,/assets/img/langfuse4-800.webp 800w,/assets/img/langfuse4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/langfuse4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/langfuse5-480.webp 480w,/assets/img/langfuse5-800.webp 800w,/assets/img/langfuse5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/langfuse5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Now that we have created the API keys, we just need to add <code class="language-plaintext highlighter-rouge">LANGFUSE_HOST</code>, <code class="language-plaintext highlighter-rouge">LANGFUSE_PUBLIC_KEY</code>, <code class="language-plaintext highlighter-rouge">LANGFUSE_SECRET_KEY</code> as environment variables and we’re ready to leverage langfuse.</li> </ul> <h4 id="if-youre-using-langchain">If you’re using Langchain</h4> <ul> <li> <p>No additional specification is needed once the environment variables are all set other than to import associated langfuse modules.</p> </li> <li> <p>For example, I made the followin example: Using the Chatllama agent using local llama2 docker image and Langchain:</p> </li> </ul> <pre><code class="language-{python}"># Import packages
from langchain_community.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage
from langchain_core.prompts import ChatPromptTemplate

from langfuse import Langfuse
from langfuse.callback import CallbackHandler

# Setup Chatllama agent
llm = ChatOllama(model="llama2")

# Setup Prompt template
text = "You are a travel agent that help people prepare travel itinerary. {question}"
prompt = ChatPromptTemplate.from_template(text)

# Chain
chain = prompt | llm | StrOutputParser()

# Langfuse configuration to create Prompt logging
langfuse = Langfuse()
langfuse.create_prompt(
    name="ollama-test-prompt",
    prompt=text,
    is_active=True,
    config = {
        "model": "llama2",
        "temperature": 0.2,
        "supported_languages": ["en"]
    }
)

# Langfuse callback handler to allow traces
langfuse_handler = CallbackHandler(
    session_id="test-1234",
    user_id = "chi-local"
)

# Invoke the chain to answer the user question
print(chain.invoke({"question": "Travel plan for 6 days Iceland travel in June."}, 
config={"callbacks": [langfuse_handler]}))

</code></pre> <ul> <li>We’d found the running under the <code class="language-plaintext highlighter-rouge">localtest</code> project.As we have specified <code class="language-plaintext highlighter-rouge">langfuse.create_prompt</code> configurations, the prompt template and model config is logged under <code class="language-plaintext highlighter-rouge">Prompts</code>:</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prompt-480.webp 480w,/assets/img/prompt-800.webp 800w,/assets/img/prompt-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/prompt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>And We’d be able to see this execution logged on Langfuse in Traces as we have setup the CallbackHandler. This supports us keep track of the input, output, latency, number of tokens, cost and more.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tracedetail-480.webp 480w,/assets/img/tracedetail-800.webp 800w,/assets/img/tracedetail-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/tracedetail.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Multiple executions under the same project can be tracked, which can used as a dashboard to support debugging for data scientists.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/traces-480.webp 480w,/assets/img/traces-800.webp 800w,/assets/img/traces-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/traces.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="llm"/><category term="learn"/><summary type="html"><![CDATA[I also explored Langfuse as an alternative to LangSmith, specifically the local host option, Self-host &gt; Local (docker compose).]]></summary></entry><entry><title type="html">LangSmith</title><link href="https://achchg.github.io/blog/2024/LLMOps/" rel="alternate" type="text/html" title="LangSmith"/><published>2024-03-19T00:00:00+00:00</published><updated>2024-03-19T00:00:00+00:00</updated><id>https://achchg.github.io/blog/2024/LLMOps</id><content type="html" xml:base="https://achchg.github.io/blog/2024/LLMOps/"><![CDATA[<p>Since I first getting into LLM integrated within App development, I’ve been mainly using logger for tracing (e.g. to keep track of module processed logs) and debugging. More recently, I tried looking into platforms that might make LLM code development more seamless. <a href="https://www.langchain.com/langsmith">LangSmith</a> was the first brought to my attention as I’ve been using Langchain in most of my usecases. LangSmith is developed as a Platform to support DevOps by Langchain. We can look for detail capabilities of LangSmith <a href="https://docs.smith.langchain.com">here</a>, but at high-level, it offers tracing, evaluation, and prompt management for LangChain (I’ve tried and will give an example below) or other LLM framework (<strong>To be tried</strong>).</p> <p>Below, I’d document some notes how to get started with LangSmith.</p> <h4 id="pre-requisite-and-setup">Pre-requisite and setup</h4> <ul> <li> <p>Steps by steps guide can be found <a href="https://docs.smith.langchain.com/setup">here</a>.</p> </li> <li> <p>For personal use, I setup a LangSmith account linking to my github account. After that, we’d see the following screen.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/image-480.webp 480w,/assets/img/image-800.webp 800w,/assets/img/image-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/image.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p></p> <ul> <li> <p>LangSmith started charging more recently, and it’s pricing model for non-personal account can be found <a href="https://docs.smith.langchain.com/pricing">here</a>. There are alternative open-sourced tools (e.g. <a href="https://langfuse.com">Langfuse</a>) that I will share in another future post.</p> </li> <li> <p>I then created my LangSmith API keys elect “Settings”. Use this API key for the env variable <code class="language-plaintext highlighter-rouge">LANGCHAIN_API_KEY</code>.</p> </li> </ul> <h4 id="if-youre-using-langchain">If you’re using Langchain</h4> <ul> <li>No additional specification is needed once the environment variable is all set. For example, if running the below Langchain example:</li> </ul> <pre><code class="language-{python}">from langchain_community.llms import Ollama
from langchain_community.chat_models import ChatOllama
from langchain_core.messages import HumanMessage

llm = Ollama(model="llama2")
chat_model = ChatOllama()

text = "What would be a good time to eat lunch?"
messages = [HumanMessage(content=text)]

print(llm.invoke(text))
</code></pre> <ul> <li>We’d found the running under the default project if without specifying:</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/image-1-480.webp 480w,/assets/img/image-1-800.webp 800w,/assets/img/image-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/image-1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p></p> <ul> <li>And We’d be able to see this execution logged on LangSmith. This supports us keep track of the input, output, latency, number of tokens, cost and more.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/image-2-480.webp 480w,/assets/img/image-2-800.webp 800w,/assets/img/image-2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/image-2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p></p> <ul> <li>Multiple executions under the same project can be tracked, which can used as a dashboard to support debugging for data scientists.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/image-3-480.webp 480w,/assets/img/image-3-800.webp 800w,/assets/img/image-3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/image-3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p></p> <ul> <li>More “how-tos” can be found <a href="https://docs.smith.langchain.com/tracing/faq">here</a>.</li> </ul>]]></content><author><name></name></author><category term="llm"/><category term="learn"/><summary type="html"><![CDATA[Since I first getting into LLM integrated within App development, I’ve been mainly using logger for tracing (e.g. to keep track of module processed logs) and debugging. More recently, I tried looking into platforms that might make LLM code development more seamless. LangSmith was the first brought to my attention as I’ve been using Langchain in most of my usecases. LangSmith is developed as a Platform to support DevOps by Langchain. We can look for detail capabilities of LangSmith here, but at high-level, it offers tracing, evaluation, and prompt management for LangChain (I’ve tried and will give an example below) or other LLM framework (To be tried).]]></summary></entry><entry><title type="html">How to run LLMs locally on your machine?</title><link href="https://achchg.github.io/blog/2024/Ollama/" rel="alternate" type="text/html" title="How to run LLMs locally on your machine?"/><published>2024-03-12T00:00:00+00:00</published><updated>2024-03-12T00:00:00+00:00</updated><id>https://achchg.github.io/blog/2024/Ollama</id><content type="html" xml:base="https://achchg.github.io/blog/2024/Ollama/"><![CDATA[<p>One of the questions I had when first started working with LLMs was around local development to support quick prototyping without worrying much of the cost (e.g. I’m still protytping for the GenAI usecase, I’d need to test the engineering pipeline multiple times which might lead to multiple prompt requests == “$$”.) or data privacy (e.g. What if I do not feel comfortable sharing my datasets, chat histories through OpenAI or Huggingface API calls to connect to remotely hosted LLMs?)</p> <p>In particular, I have experienced trying out <a href="https://gpt4all.io/index.html">GPT4ALL</a> and <a href="https://ollama.com">Ollama</a>. Here are some documentations:</p> <h4 id="gpt4all-github">GPT4ALL (<a href="https://github.com/nomic-ai/gpt4all">Github</a>)</h4> <ul> <li> <p>This was the very first approach I tried. It was easy to setup, and it also provides a chat client (front end component) available for downloads. I only tried the backend API by installing gpt4all and langchain. After that, we can download and try different LLMs model on the GPT4ALL leaderboard.</p> </li> <li>We can directly run GPT4 model with python using gpt4all (<a href="https://docs.gpt4all.io">documentation</a>) or using Langchain (<a href="https://python.langchain.com/docs/integrations/llms/gpt4all">documentation</a>): <pre><code class="language-{python}">from langchain_community.llms import GPT4All
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

local_path = 'path to where you save the model bin file'
llm = GPT4All(model=local_path, callbacks=[StreamingStdOutCallbackHandler()])

template = """{question}"""

prompt = PromptTemplate.from_template(template)
llm_chain = LLMChain(prompt=prompt, llm=llm)
question = "What would be a good time to eat lunch?"

llm_chain.run(question)
</code></pre> </li> <li><strong>2024-03-18 edits</strong>: The above code would throw a pydantic validation error (<a href="https://github.com/langchain-ai/langchain/issues/7778">similar issues raise here</a>). With a few research online, this might be due to compatibiilty issue. The above code used to work on <code class="language-plaintext highlighter-rouge">pydantic==1.10.0</code>, <code class="language-plaintext highlighter-rouge">langchain==0.0.320</code> and <code class="language-plaintext highlighter-rouge">gpt4all==2.0.0</code>. However, now my environmet is using <code class="language-plaintext highlighter-rouge">pydantic=2.6.4</code> and <code class="language-plaintext highlighter-rouge">langchain==0.1.12</code>. <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pydantic.v1.error_wrappers.ValidationError: 1 validation error for GPT4All __root__ -&gt; __root__
Serializable.__init__() takes 1 positional argument but 2 were given (type=type_erro
</code></pre></div> </div> <p>Therefore, it seems further experiment and research on pacakge versions is needed before using gpt4all.</p> </li> </ul> <h4 id="ollama-github">Ollama (<a href="https://github.com/ollama/ollama">Github</a>)</h4> <ul> <li> <p>Very easy to setup. I followed the quickstart steps to install ollama on my Mac, and then run ollama docker image to start my local container. After these steps, I’m able to pull or run different LLMs available on their leaderboard.</p> </li> <li>We can directly run Ollama models from Terminal after setup: <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ollama run llama2 "What would be a good time to eat lunch?"
</code></pre></div> </div> </li> <li>If need to further develop with python, it is also well integrated with Langchain: <pre><code class="language-{python}">from langchain_community.llms import Ollama
from langchain_community.chat_models import ChatOllama
from langchain_core.messages import HumanMessage

llm = Ollama(model="llama2")
chat_model = ChatOllama()

text = "What would be a good time to eat lunch?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
</code></pre> </li> <li>If would like to use Ollama with a front-end interface, there are other github repos that have already implemented integration that we can leverage. I’ll document my findings in another journal!</li> </ul> <p>In general, I found the Ollama setup more easy to kick-off and to start my own experiments locally.</p>]]></content><author><name></name></author><category term="llm"/><category term="learn"/><summary type="html"><![CDATA[One of the questions I had when first started working with LLMs was around local development to support quick prototyping without worrying much of the cost (e.g. I’m still protytping for the GenAI usecase, I’d need to test the engineering pipeline multiple times which might lead to multiple prompt requests== “$$”.) or data privacy (e.g. What if I do not feel comfortable sharing my datasets, chat histories through OpenAI or Huggingface API calls to connect to remotely hosted LLMs?)]]></summary></entry><entry><title type="html">NBeats</title><link href="https://achchg.github.io/blog/2022/NBeats/" rel="alternate" type="text/html" title="NBeats"/><published>2022-11-15T00:00:00+00:00</published><updated>2022-11-15T00:00:00+00:00</updated><id>https://achchg.github.io/blog/2022/NBeats</id><content type="html" xml:base="https://achchg.github.io/blog/2022/NBeats/"><![CDATA[<p>Finally got chance to jog down some notes around the <a href="https://arxiv.org/pdf/1905.10437.pdf">N-BEATS</a> paper.</p> <h4 id="main-contributions">Main contributions</h4> <ul> <li>1) <strong>deep neural architecture</strong>: Pure deep learning network without time-series specific componets that performed better than well-established statistical models on reference time-series datasets (e.g. M3, M4, etc.).</li> <li>2) <strong>interpretable DL for time series</strong></li> </ul> <h4 id="algorithm">Algorithm</h4> <ul> <li><strong>Task:</strong> Predict the vector of future values (\(\mathbb{y} = [y_{T+1}, y_{T+2}, ..., y_{T+H}]\)) of length H forecast horizon given a length T observed historical series \([y_{1}, y_{2}, ..., y_{T}]\). <ul> <li>if define a lookback window of length t \(\leq\) T from \(y_T\), the model period can be denoted as \(\mathbb{x} = [y_{T-t+1}, y_{T-t+2}, ..., y_{T}]\)</li> <li>the forecasts are \(\hat{\mathbb{y}}\)</li> <li>Common evaluation metrics for time-series forecasts are: <ul> <li><strong>MAPE (mean absolute percentage error):</strong> \(\frac{100}{H}\sum_{i=1}^H \frac{\left| y_{T+i}-\hat{y}_{T+i} \right| }{|y_{T+i}|}\) (the errors are scaled by the ground truth)</li> <li><strong>SMAPE (symmetric MAPE):</strong> \(\frac{200}{H}\sum_{i=1}^H \frac{\left| y_{T+i}-\hat{y}_{T+i} \right| }{|y_{T+i}| + |\hat{y}_{T+i}|}\) (the errors are scaled by the average of forecast and ground truth)</li> <li><strong>MASE (mean absolute scaled error):</strong> \(\frac{1}{H}\sum_{i=1}^H \frac{\left| y_{T+i}-\hat{y}_{T+i} \right| }{\frac{1}{T+H-m} \sum_{j=m+1}^{T+H}|y_j - y_{j-m}|}\) (the errors are scaled by the average error measured m periods in the past, accounting for seasonality)</li> </ul> </li> </ul> </li> <li><strong>Architecture:</strong></li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nbeats-480.webp 480w,/assets/img/nbeats-800.webp 800w,/assets/img/nbeats-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/nbeats.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="nbeats image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>Basic block: (blue figure)</strong> Take 1 Block input vector (\(x_l\)) and output 2 vectors: \(\hat{x_l}\), the backcast (usually with length of 2H - 7H) + \(\hat{y_l}\), the forecast (usually with length of H) <ul> <li>Block 1’s input is the overall model input (\(x_l\)): lookback values within the defined window (\(x_l\))</li> <li>Block 2 - L’s input is the the backcast <strong>residual</strong> values from the previous layer (\(x_l\) = \(x_{l-1} - \hat{x}_{l-1}\))</li> <li>Within the Block, the default algorithm is consist of 4 standard fully-connected (FC) layers with ReLU non-linearity stacking in sequence with 2 linear output layers.</li> </ul> </li> <li><strong>Doubly residual stacking (orange+yellow figure):</strong> An extension from the classic <a href="https://achchg.github.io/blog/2022/ResNet/">ResNet</a> architecture, e.g. Input vector (x) is added to the output vector (F(x)) before passing to the next stack. The proposed architecture involved the two residual branches (forecast and backcast) as described in above section within each block, and stack the residues between blocks with the following two equations: <ul> <li>Backcast residual branch, each depending on residues from previous block: \(x_l\) = \(x_{l-1} - \hat{x}_{l-1}\)</li> <li>Forecast residual branch, summation of forecasts by all blocks within a stack: \(\hat{y} = \sum_l \hat{y}_l\)</li> <li>Both equations are repeated as the same architecture across stacks and the final forecasts are the summation of \(\hat{y}\)</li> </ul> </li> <li>Aboves were notes over a generic model setup <strong>without</strong> time-series specific knowledge on none-linear trend or seasonality (a.k.a the assumptions on \(g_b\) and \(g_f\) specified in the cartoon figure were linear). That says, with additional assumptions made on the two equations across blocks/stacks, we’re also able to incorporate TS-specific assumptions in for tuning.</li> </ul> <p>Helpful videos &amp; blogs:</p> <ul> <li><a href="https://www.youtube.com/watch?v=ZILIbUvp5lk">ResNets</a> and <a href="https://www.youtube.com/watch?v=RYth6EbBUqM&amp;list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&amp;index=15">Why ResNets work</a> by DeepLearningAI</li> <li><a href="https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035">An Overview of ResNet and its Variants</a></li> </ul>]]></content><author><name></name></author><category term="dl"/><category term="learn"/><summary type="html"><![CDATA[Day 55]]></summary></entry><entry><title type="html">SQL</title><link href="https://achchg.github.io/blog/2022/SQL/" rel="alternate" type="text/html" title="SQL"/><published>2022-10-25T00:00:00+00:00</published><updated>2022-10-25T00:00:00+00:00</updated><id>https://achchg.github.io/blog/2022/SQL</id><content type="html" xml:base="https://achchg.github.io/blog/2022/SQL/"><![CDATA[<p>Helpful SQL practice resources:</p> <h1 id="knowledge-refreshes">Knowledge refreshes</h1> <ul> <li><a href="https://e2eml.school/sql_resources.html">e2eml.school</a> has a lot of great resources shared</li> <li><a href="https://learn.udacity.com/courses/ud198">SQL for Data Analysis</a> by Udacity includes basic to advanced SQL tutorials</li> <li><a href="https://www.youtube.com/watch?v=zrCLRC3Ci1c">Lecture 7: SQL</a> by CS50</li> <li><a href="https://www.interviewquery.com/p/data-science-sql-interview-questions">Top 25+ Data Science SQL Interview Questions</a></li> </ul> <h1 id="practice-examples">Practice examples</h1> <ul> <li><a href="https://leetcode.com/study-plan/sql/?progress=xanj57th">Leetcode Study Plan</a></li> <li><a href="https://sqlbolt.com">SQLBolt</a></li> </ul>]]></content><author><name></name></author><category term="sql"/><category term="review"/><summary type="html"><![CDATA[Day 33]]></summary></entry><entry><title type="html">Attention</title><link href="https://achchg.github.io/blog/2022/Attention/" rel="alternate" type="text/html" title="Attention"/><published>2022-10-24T00:00:00+00:00</published><updated>2022-10-24T00:00:00+00:00</updated><id>https://achchg.github.io/blog/2022/Attention</id><content type="html" xml:base="https://achchg.github.io/blog/2022/Attention/"><![CDATA[<p>There are a lot of nice materials explaining Attention model fairly well. My favorite have been <a href="https://distill.pub/2016/augmented-rnns/#attentional-interfaces">the “Attention Interfaces” of this blog post</a> and the <a href="https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf">CS224N lecture note</a>.</p> <h4 id="attention">Attention</h4> <p>Great <a href="https://www.youtube.com/watch?v=SysgYptB198">video</a> with the intuition of Attention model explained by Andrew Ng.</p> <p>Typical sequence-to-sequence models like RNN were used in machine translation, where a input sentence of language A is translated to an output sentence of target language B in an encoder-decoder architecture. One problem with it was that the models go word by word within a sentence during encoding and depending on the final hidden layer before decoding to memorize everything fed into the system for translation; the decoder than taking the hidden layer and pass on another sequence to predict the most likely word that should pop up next given the current translated word.</p> <p>On the contrary, the decoder network (language B) of the Attention model was trained by <strong>the entire input sequence</strong> (language A) at every decoding step (y_t) with the goal of learning the <strong>attention weight</strong> (\(\alpha_{t,1}\) to \(\alpha_{t,T}\)) of individual translated word in B on all the input words (x_1, …, x_T) in A. Below cited the Figure 1 from the <a href="https://arxiv.org/pdf/1409.0473.pdf">original Attention paper</a>.</p> <div class="row"> <div class="col-md-3 offset-md-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention-480.webp 480w,/assets/img/attention-800.webp 800w,/assets/img/attention-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attention.png" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Great video materials:</p> <ul> <li><a href="https://www.youtube.com/watch?v=quoGRI-1l0A">Attention model</a> by Andrew Ng.</li> </ul>]]></content><author><name></name></author><category term="ml,"/><category term="dl"/><category term="review"/><summary type="html"><![CDATA[Day 32]]></summary></entry><entry><title type="html">Gradient boosting</title><link href="https://achchg.github.io/blog/2022/Gradient_boosting/" rel="alternate" type="text/html" title="Gradient boosting"/><published>2022-10-14T00:00:00+00:00</published><updated>2022-10-14T00:00:00+00:00</updated><id>https://achchg.github.io/blog/2022/Gradient_boosting</id><content type="html" xml:base="https://achchg.github.io/blog/2022/Gradient_boosting/"><![CDATA[<p>While looking at <a href="https://achchg.github.io/blog/2022/ResNet/">ResNets</a>, One thought came to my mind - “So isn’t this boosting?”. An immediate next step of researching this topic led me to this stackoverflow <a href="https://stats.stackexchange.com/questions/214273/are-residual-networks-related-to-gradient-boosting">discussion</a>, and later a review session of gradient boosting algorithms.</p> <h4 id="gradient-boosting">Gradient boosting</h4> <p>Actually the Wikipedia <a href="https://en.wikipedia.org/wiki/Gradient_boosting">page</a> of gradient boosting summarizes the algorithm fairly well. High-level description of what it is with my own words:</p> <p><strong>“An additive model of weaker trees with forward feature selection trained by gradient descent method that aims to learn to avoid making errors made in previous stages (trees).”</strong></p> <p>How gradient boosting works:</p> <ol> <li> <p>Define a boosting tree (\(F_M\)) that we aim for \(M\) stages (M weak learners/trees): \(\hat{F}_M(x_i) = \Sigma_{m=1}^M \alpha_m h_m(x_i) + \text{const.}\) where \(\alpha_m\) is the learning rate.</p> </li> <li>Define loss at the \(m^{th}\) stage/base learner: <ul> <li>loss function: \(L(y, F_m(x))\)</li> <li>example base learner: \(h_m(x_i) = y_i - \hat{y}_{i,m} = y_i - F_m(x_i)\)</li> <li>goal is to minimize the loss</li> </ul> </li> <li> <p>At stage 0, as there was no stage before it. Therefore, in the very first tree, we are fitting the tree with \(y_i\) directly: \(F_0(x_i) = \arg \min_\alpha \Sigma_{i=1}^n L(y_i, \alpha)\)</p> </li> <li> <p>At stage \(m\) (where \(m \neq 0\)), we are fitting the \(m^{th}\) tree with the residual: \(F_m(x_i) = F_{m-1}(x_i) + \arg \min_{h_m} \Sigma_{i=1}^n L(y_i, F_{m-1}(x_i) + \alpha h_m(x_i))\)</p> </li> <li>Repeat #4 and keep updating the model until convergence (\(F_m(x_i) = F_{m-1}(x_i) + \alpha_mh_m(x_i)\)).</li> </ol> <p>Also, nicely explained source for boosting algorithm by <a href="https://c.d2l.ai/stanford-cs329p/_static/pdfs/cs329p_slides_7_3.pdf">CS 329P : Practical Machine Learning (2021 Fall)</a>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre> <span class="k">class</span> <span class="nc">GradientBoosting</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">base_learner</span><span class="p">,</span> <span class="n">n_learners</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">learners</span> <span class="o">=</span> <span class="p">[</span><span class="nf">clone</span><span class="p">(</span><span class="n">base_learner</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_learners</span><span class="p">)]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">learner</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">learners</span><span class="p">:</span>
            <span class="n">learner</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">residual</span><span class="p">)</span>
            <span class="n">residual</span> <span class="o">-=</span> <span class="n">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">learner</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="p">[</span><span class="n">learner</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">learner</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">learners</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">preds</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">lr</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Here, we can leverage different base_learner (e.g. regression or classification models with differen objective functions)</p> <h5 id="gradient-boosting-regression">Gradient boosting regression</h5> <p><strong>Loss function:</strong> MSE!</p> \[\begin{align*} L(y, F_m(x_i)) &amp; = \frac{1}{n}\Sigma_{i=1}^n(F_m(x_i)-y_i)^2\\ \arg \min_{F_m} \Sigma_{i=1}^n L(y_i, F_m) &amp; = -\frac{\partial L(y, F_m(x_i))}{\partial F_m} \propto \Sigma_{i=1}^n(y_i-F_m(x_i)) \rightarrow \boxed{r_m = \text{pseudo-residual}}\\ \arg \min_{\gamma} L(y, F_{m-1}(x_i)+\gamma) &amp;\approx \arg \min_{\gamma} [L(y, F_{m-1}(x_i)) +\frac{\partial L(y, F_{m-1}(x_i))}{\partial F}\gamma +\frac{1}{2}\frac{\partial^2 L(y, F_{m-1}(x_i))}{\partial F^2}\gamma^2]\\ &amp; = 0 + \frac{\partial L(y, F_{m-1}(x_i))}{\partial F} + \frac{\partial^2 L(y, F_{m-1}(x_i))}{\partial F^2}\gamma \stackrel{\text{set}}{=} 0\\ \gamma &amp; = - \frac{\frac{\partial L(y, F_{m-1}(x_i))}{\partial F}}{\frac{\partial^2 L(y, F_{m-1}(x_i))}{\partial F^2}} = \boxed{\frac{\Sigma_{i=1}^n y_i - F_m(x_i)}{n}} \end{align*}\] <p><strong>Source <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor">code</a> from sklearn</strong></p> <p>Great video materials:</p> <ul> <li><a href="https://www.youtube.com/watch?v=2xudPOBz-vs">StatQuest</a></li> </ul> <h5 id="gradient-boosting-classification">Gradient boosting classification</h5> <p><strong>Loss function:</strong> Can use the same as logistic regression, details illustrated <a href="https://achchg.github.io/blog/2022/Stochastic_gradient_descent/">here</a>.</p> \[\begin{align*} L(y, F_m(x_i)) &amp; = \Pi_{i=1}^n F_m(x_i)^y_i(1-F_m(x_i))^{1-y_i}\\ \ell(y, F_m(x_i)) &amp; = \Sigma_{i=1}^n [y_i\log(F_m(x_i)) + (1-y_i)\log(1-F_m(x_i))] \\ &amp; = \Sigma_{i=1}^n y_i\log(\frac{F_m(x_i)}{1-F_m(x_i)}) + \log(1-F_m(x_i))\\ &amp; = \Sigma_{i=1}^n y_i\log(\text{Odds}) + \log(1-\frac{\exp(\log(\text{Odds}))}{1+\exp(\log(\text{Odds}))})\\ &amp; = \Sigma_{i=1}^n y_i\log(\text{Odds}) - \log(1+\exp(\log(\text{Odds})))\\ \arg \min_{\log(\text{Odds})} \ell(y, F_m(x_i)) &amp;= \frac{\partial \ell(y, F_m(x_i))}{\partial \log(\text{Odds})} = \Sigma_{i=1}^n y_i - \frac{\exp(\log(\text{Odds}))}{1+\exp(\log(\text{Odds}))} \\ &amp;= \Sigma_{i=1}^n y_i - F_m(x_i) \rightarrow \boxed{r_m = \text{pseudo-residual}}\\ \arg \min_{\gamma} \ell(y, F_{m-1}(x_i)+\gamma) &amp;\approx \arg \min_{\gamma} [\ell(y, F_{m-1}(x_i)) +\frac{\partial \ell(y, F_{m-1}(x_i))}{\partial F}\gamma +\frac{1}{2}\frac{\partial^2 \ell(y, F_{m-1}(x_i))}{\partial F^2}\gamma^2]\\ &amp; = 0 + \frac{\partial \ell(y, F_{m-1}(x_i))}{\partial F} + \frac{\partial^2 \ell(y, F_{m-1}(x_i))}{\partial F^2}\gamma \stackrel{\text{set}}{=} 0\\ \gamma &amp; = - \frac{\frac{\partial \ell(y, F_{m-1}(x_i))}{\partial F}}{\frac{\partial^2 \ell(y, F_{m-1}(x_i))}{\partial F^2}} = \boxed{\frac{\Sigma_{i=1}^n y_i - F_m(x_i)}{\Sigma_{i=1}^n F_m(x_i)*(1-F_m(x_i))}} \end{align*}\] <p><strong>Source <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier">code</a> from sklearn</strong></p> <p>Great video materials:</p> <ul> <li><a href="https://www.youtube.com/watch?v=StWY5QWMXCw">StatQuest</a></li> </ul> ]]></content><author><name></name></author><category term="ml,"/><category term="boosting,"/><category term="gradient"/><category term="review"/><summary type="html"><![CDATA[Day 22]]></summary></entry><entry><title type="html">ResNets</title><link href="https://achchg.github.io/blog/2022/ResNet/" rel="alternate" type="text/html" title="ResNets"/><published>2022-10-10T00:00:00+00:00</published><updated>2022-10-10T00:00:00+00:00</updated><id>https://achchg.github.io/blog/2022/ResNet</id><content type="html" xml:base="https://achchg.github.io/blog/2022/ResNet/"><![CDATA[<p>I recently found a paper regarding time-series forecasting: <a href="https://arxiv.org/pdf/1905.10437.pdf">N-BEATS</a> and found myself missing a few pre-requsite concept. One of them was the use of classic residual network algorithm, first proposed as <a href="https://arxiv.org/pdf/1512.03385.pdf">ResNets</a>. Here are some of my learning notes:</p> <h4 id="resnets">ResNets</h4> <p>A residual neural network (<a href="https://en.wikipedia.org/wiki/Residual_neural_network">ResNet</a>) is a deep neural netword architecture, which uses skip connections/shortcuts to jump over some layers (usually 2-3 layer skips) to avoid the problems of:</p> <ul> <li>1) <strong>vanishing/exploding gradients</strong>: gradients becoming too small or big when increasing layers, and</li> <li>2) <strong>degradation</strong>: deeper NN has larger training/testing error.</li> </ul> <p>ResNets contain typical NN characteristics of adding nonlinearities (ReLU) and batch normalization in between the layers. Note that the residual (\(F(x)\)) of a residual block will be add to an identity matrix (\(x\)) before passing on to the ReLU activation function.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/residual_block-480.webp 480w,/assets/img/residual_block-800.webp 800w,/assets/img/residual_block-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/residual_block.png" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Why it solves the 2 problems above?</p> <ul> <li>If considering a NN above, then \(F(x) + x = a^{[2]} = g(z^{[2]} + x) = g(w^{[2]} a^{[1]} + b^{[2]} + x)\).</li> <li>To solve the above equation by minimizing \(F(x)\) as 0, we’d get \(a^{[2]} = x \approx g(x)\), where we expect \(w^{[2]} \approx 0\) and \(b^{[2]} \approx 0\) so as \(w^{[l]}\) and \(b^{[l]}\) at earlier layers (l).</li> </ul> <p>Example pytorch resource for ResNet18 is <a href="https://pytorch.org/hub/pytorch_vision_resnet/">here</a></p> <p>Original ResNets paper is <a href="https://arxiv.org/pdf/1512.03385.pdf">here</a>.</p> <p>Helpful videos &amp; blogs:</p> <ul> <li><a href="https://www.youtube.com/watch?v=ZILIbUvp5lk">ResNets</a> and <a href="https://www.youtube.com/watch?v=RYth6EbBUqM&amp;list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&amp;index=15">Why ResNets work</a> by DeepLearningAI </li> </ul>]]></content><author><name></name></author><category term="dl"/><category term="image"/><category term="learning"/><summary type="html"><![CDATA[Day 18]]></summary></entry><entry><title type="html">RNN/LSTM</title><link href="https://achchg.github.io/blog/2022/RNN/" rel="alternate" type="text/html" title="RNN/LSTM"/><published>2022-10-07T00:00:00+00:00</published><updated>2022-10-07T00:00:00+00:00</updated><id>https://achchg.github.io/blog/2022/RNN</id><content type="html" xml:base="https://achchg.github.io/blog/2022/RNN/"><![CDATA[<p>My notes from reviewing RNN model in NLP, following the flow of <a href="https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf">RNN</a> and <a href="https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf">Seq2seq</a> lecture notes.</p> <h4 id="rnn-architecture">RNN architecture</h4> <p>I’ll use the below figure cited from CS224n lecture 5-6 notes of RNN to summarize RNN:</p> <p>“The elements (e.g. words) were fed into the algorithm one after one along (e.g. the \(t^{th}\) element of the input sequence, \(w_{t}\)) with the hidden output layer (\(h_{t-1}\)) from the previous timestamp (\(t-1\)) in predicting the most likely next element of the output sequence (\(y_{t}\))”</p> \[\begin{align*} h_t &amp; = \sigma(W^{(hh)}h_{t−1} + W^{(hx)}x_{t})\\ y_t &amp; = \text{softmax}(W^{(S)}h_t) \end{align*}\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rnn-480.webp 480w,/assets/img/rnn-800.webp 800w,/assets/img/rnn-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rnn.png" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Parameters to be solved: \(h_t\), \(W^{(hh)}\), \(W^{(hx)}\) and \(W^{(S)}\)</li> <li>With the softmax activation function, the common loss function for RNN is the cross-entropy loss (as derived previously in the <a href="https://achchg.github.io/blog/2022/Stochastic_gradient_descent/">SGD post with logistic regression</a>):</li> </ul> \[\begin{align*} J^{(t)}(\theta) &amp;= −\Sigma_{j=1}^{|V|}y_{t,j} * log(\hat{y}_{t,j}) \\ J(\theta) &amp;= \frac{1}{T}\Sigma_{t=1}^TJ^{(t)}(\theta) \end{align*}\] <ul> <li>As the sequence is getting longer, there could be vanishing/exploding gradient problem. As we’d solve for the gradient for \(J^{(t)}(\theta)\) with chain rule with respect to multiple layers of t. The gradient can become very small or large. One way to solve the exploding gradient problem is to setup a threshold for the gradient, as the gradient exceeds the threshold, adjust the gradient to a smaller level.</li> <li>Evaluation method of RNN model: <strong>Perplexity!</strong> The lower the perplexity, the better the model.</li> </ul> \[\begin{align*} \text{perplexity} = \exp({J}) \end{align*}\] <h4 id="long-short-term-memory-lstm">Long Short Term Memory (LSTM)</h4> <p>Here is a very nicely written <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">blog</a> and where I referenced the below figure, but I’d take my own notes below:</p> <p>“LSTM is a special form of RNN that aimed to avoid the long-term dependency problem due to long sequence.”</p> \[\begin{align*} \color{Cerulean}\text{forget gate: } &amp; \color{Cerulean}\boxed{f^{(t)} = \sigma(W_fh_{t−1} + U_fx_{t} + b_f)}\\ &amp;\color{Cerulean}\textnormal{what content from t-1 is kept, $f^{(t)}$ is between 0 and 1; larger means "memory keeping"}\\ \color{blue}\text{input gate: } &amp; \color{blue}\boxed{i^{(t)} = \sigma(W_fi_{t−1} + U_ix_{t} + b_i)}\\ \color{darkblue}\text{output gate: } &amp; \color{darkblue}\boxed{o^{(t)} = \sigma(W_fo_{t−1} + U_ox_{t} + b_0)}\\ \\ \color{Lavender} \text{new cell content: } &amp; \color{Lavender}\boxed{\tilde{c}_{t} = \tanh(W_ch_{t-1} + U_cx_{t} + b_c)}\\ \color{Purple} \text{new cell state: } &amp; \color{Purple}\boxed{c_{t} = f_{t} \cdot c_{t-1} + i_t \cdot \tilde{c}_{t}} \\ &amp;\color{Purple}\textnormal{new and carryover contents to be written}\\ \\ &amp; h_t = o_t \cdot \tanh c_t\\ &amp;\textnormal{new memory to be ouput}\\ \end{align*}\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lstm-480.webp 480w,/assets/img/lstm-800.webp 800w,/assets/img/lstm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/lstm.png" width="100%" height="auto" title="lstm image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Nice RNN resources:</p> <ul> <li><a href="https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture05-rnnlm.pdf">CS224N lecture 5</a></li> <li><a href="https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture06-fancy-rnn.pdf">CS224N lecture 6</a></li> <li><a href="https://www.youtube.com/watch?v=AsNTP8Kwu80">StatQuest</a></li> </ul>]]></content><author><name></name></author><category term="dl"/><category term="sequence"/><category term="review"/><summary type="html"><![CDATA[Day 15]]></summary></entry><entry><title type="html">Adam (adaptive moment estimation) optimizer</title><link href="https://achchg.github.io/blog/2022/Adam_optimizer/" rel="alternate" type="text/html" title="Adam (adaptive moment estimation) optimizer"/><published>2022-10-04T00:00:00+00:00</published><updated>2022-10-04T00:00:00+00:00</updated><id>https://achchg.github.io/blog/2022/Adam_optimizer</id><content type="html" xml:base="https://achchg.github.io/blog/2022/Adam_optimizer/"><![CDATA[<p>Reviewing <a href="https://web.stanford.edu/class/cs224n/assignments/a3_handout.pdf">week 3 assignment</a> of NLP with deep learning brought Adam optimization algorithm back to my attention. Here I’d summarize what I did in my homework 3 for my future reference:</p> <h4 id="adam-optimizer">Adam optimizer</h4> <p>From standard SGD, we would use a mini-batch (e.g. single sample) of data in the update rule below for updating the \(J(\theta)\):</p> \[\theta := \theta - \alpha \nabla_\theta J(\theta)\] <p>where \(\alpha\) is the learning rate and \(\nabla_\theta J(\theta)\) represent the partial derivatives of the cost function wrt \(\theta\).</p> <p>Adam optimization, in addition, takes 2 additional steps beyond SGD:</p> <h5 id="update-biased-first-order-moment-estimate">Update biased first order moment estimate</h5> \[\begin{align*} m &amp; := \beta_1 m + (1 − \beta_1)\nabla_\theta J(\theta)\\ \theta &amp; := \theta - \alpha m \end{align*}\] <p>As \(m\) is set as a weighted average of the rolling gradient average of the <strong>previous</strong> iterations and the gradient of the <strong>current</strong> iteration, we can expect the <strong>momentum</strong> step making the gradient descent update smoother than that of the SGD (which only consider the <strong>current</strong> iteration). The current gradient (\(\nabla_{\theta} J(\theta)\)) will be weighted larger than the individual gradients after k (\(\frac{\beta_1}{1-\beta_1}\)) iterations, as the individual previous gradients has a weight of \(\frac{\beta_1}{k}\) (where k = num of past iterations).</p> <h5 id="update-biased-second-order-raw-moment-estimate">Update biased second order raw moment estimate</h5> \[\begin{align*} v &amp; := \beta_2 v + (1 − \beta_2)(\nabla_\theta J(\theta) \odot \nabla_\theta J(\theta))\\ \theta &amp; := \theta - \alpha \frac{m}{\sqrt{v}} \end{align*}\] <p>As \(m\) is divided by the \(\sqrt{v}\) (the gradient of Adam), the gradients that have smaller gradient magnitude (\(v\)) will get larger updates. As \(v\) is derived from squared of \(\nabla_{\theta} J(\theta)\), \(v\) is usually associated with smaller gradient. This might help further smoothing out the gradient descent from the <strong>momentum</strong> step, by giving larger weights to the smaller gradients when the SGD update does not guarantee continuous descending in the gradients.</p> <h5 id="note-method-of-moment">Note: Method of moment</h5> <p><a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)">Method of moment</a> in statistics implies the following:</p> <p>The \(k^{th}\) moment of a random variable X with its pdf, \(f(x)\) can be expressed as:</p> \[E(X^k) = \int_X x^k f(x) dx\] <p>Therefore, the first moment of X is \(E(X)\), which is the mean of the distribution; and the second moment of X is \(E(X^2)\), which is the sum of mean squared and the variance (\(\text{Var}(X) = E(X^2) - E(X)^2\)). </p> <p>Original Adam paper is <a href="https://arxiv.org/pdf/1412.6980.pdf">here</a>; Helpful <a href="https://gregorygundersen.com/blog/2020/04/11/moments/">documentation</a> of moment statistics.</p>]]></content><author><name></name></author><category term="optimization"/><category term="gradient"/><category term="review"/><summary type="html"><![CDATA[Day 12]]></summary></entry></feed>