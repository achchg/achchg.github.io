<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Centralizing Real World Data with Machines | Chi-Hsuan Chang </title> <meta name="author" content="Chi-Hsuan Chang"> <meta name="description" content="This post was summarized from my final project @Stanford CS229."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://achchg.github.io/projects/1_project/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Centralizing Real World Data with Machines",
            "description": "This post was summarized from my final project @Stanford CS229.",
            "published": "December 05, 2021",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Chi-Hsuan</span> Chang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Centralizing Real World Data with Machines</h1> <p>This post was summarized from my final project @Stanford CS229.</p> </d-title> <d-article> <p>The paper aimed to propose a framework that leverages machine learning methods to utilize information from multiple data sources, with the ultimate goal being able to generate a de-biased data layer that allows health data scientists/researchers to perform analyses on.</p> <p>As an demonstration of the concept, I assumed a hypothetical goal:</p> <p><strong>To estimate the share of a particular item (A) against a list of competing items, possibly given a set of features.</strong></p> <p>This looks like a typical problem we’d solve with statistical inference. I tried to tackle the prompt with the following three perspectives and their impact on the need of data centralization:</p> <ol> <li> <strong>Data biasness:</strong> when multiple unbiased/biased real-world datasets are available, is a direct pooling of all datasets just always better?</li> <li> <strong>Model biasness:</strong> when applying algorithms in different data scenarios, what is the scale of model prediction error on top of the data biasness?</li> <li> <strong>Synthetic biasness:</strong> whether leveraging Generative Adversarial Networks (GAN) could generate synthetic datasets that allows us making direct unbiased model inference generated learning from biased data sources compared to baseline models</li> </ol> <h3 id="simulated-data">Simulated Data</h3> <p>We simulated different real world data scenarios leveraging the Fashion MNIST dataset <d-cite key="xiao2017fashionmnist"></d-cite> because of its high-dimensional feature space and that the target variable being multi-class.</p> <ul> <li>Overall, the dataset contains 60,000 samples with 28x28 dimension that describe the pixel graph of an individual item sample; in which we assumed these data as the true population distribution of the market (if we know what the true is).</li> <li>As illustrated in <strong>Table 1</strong>, We assumed “T-shirt” as the target product item (A) of interest. There were eight other competitor products on the market and one additional group as “All others”. We’d discuss the paper in terms of Product A vs. competitor products. With the true target market share of interest being 10%.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/table1_gan-480.webp 480w,/assets/img/table1_gan-800.webp 800w,/assets/img/table1_gan-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/table1_gan.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 1. Data &amp; selection bias assumptions within individual table. </div> <p>For each study scenario, We sampled three datasets (<strong>DS-1</strong>, <strong>2</strong>, <strong>3</strong>) randomly with different size assumptions setting specific seed from the true population. Random Gaussian noise within individual datasets was assumed as the embedded data batch effect beyond the random sample noises. Selection biases within individual datasets were assumed followed the specifying distribution in <strong>Table 1</strong>.</p> <h4 id="unbiased-datasets-with-random-gaussian-noise">Unbiased datasets with random Gaussian noise</h4> <p>To illustrate the underlying research question and the unbiased sample dataset in equation:</p> \[Y_{j}^{(i)} \sim b_{data_j}^{(i)} + \beta X^{(i)} + \epsilon^{(i)}_j\] <p>where</p> <p>\(Y^{(i)}_j \sim \textbf{Multinomial}(n, p_1, ..., p_{10})\) <strong>:</strong> n is sample size, and $p_1$ to $p_{10}$ are proportions of the 10 product labels</p> <p>\(X^{(i)}_j \sim\) Fashion MNIST (28x28) feature space</p> <p>\(\epsilon^{(i)}_j \sim\) Fashion MNIST (28x28) sample noise</p> <p>\(b_{data_j} \sim \mathcal{N}(0, \Sigma_{data_j})\)<strong>:</strong> assumed batch-effect ($\Sigma_{data_1}$, $\Sigma_{data_2}$, $\Sigma_{data_3}$) $=$ (0.1, 0.2, 0.1)</p> <h4 id="biased-datasets-with-random-gaussian-noise">Biased datasets with random Gaussian noise</h4> <p>Additional category selection biasness was assumed and specified in \textbf{Table 1}, which was illustrated as variable $Z_j$ here:</p> \[Y_{j}^{(i)} \sim b_{data_j}^{(i)} + \beta X^{(i)} + Z_j^{(i)} + \epsilon^{(i)}_j\] <p>where</p> <p>\(Z_j^{(i)} | K = k \sim \textbf{Bernouli}(p_{k,j})\)<strong>:</strong> k is the hidden category in real world that attributed to selection bias within a given data set j (if $Z_j$ = 1 then sample was observed); \(\mbox{Item}_1 ... \mbox{Item}_{10}\) were summarized by category K without additional noise.</p> <p>The biased datasets were aimed to simulate the real world scenarios that datasets observed are often non-random subset of the true population of interest. Specifically, <strong>DS-1</strong> was assumed as a biased set toward non-target classes; <strong>DS-2</strong> as an unbiased set and <strong>DS-3</strong> as a biased set toward target class.</p> <h3 id="methods">Methods</h3> <p><strong>Algorithms that predict the outcome of interest.</strong> This component aimed to study the scenario when product labels are unknown and a model is used to predict the product label. Different state-of-the-art approaches were studied here given embeded data biasness and batch noise. Max absolute scaling <d-cite key="scikit-learn"></d-cite> was implemented before fitting the models.Specifically, a logistic regression without adjusting data group and a logistic regression adjusted with additive data group effect as categorical variable.</p> <p><strong>The algorithm that generates the centralized data distribution and predict the outcome of interest.</strong> We utilized and modified a GAN module (<a href="https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/sgan/sgan.py" rel="external nofollow noopener" target="_blank">SGAN</a>) from an open-sourced Github package (Pytorch-GAN) <d-cite key="pytorchgan"></d-cite> in Pytorch <d-cite key="NEURIPS2019_9015"></d-cite>.</p> <p>SGAN is a semi-supervised GAN model that expanded from the orignal GAN model <d-cite key="goodfellow"></d-cite> which are aiming to predict both the class label (e.g. K classes, where k = 10 in our example) and whether the data is real or generated.<d-cite key="odena2016semisupervised"></d-cite> To summarize the default SGAN model, the discriminator loss was separated into an adversarial (a sigmoid activation function to classify fake vs. real) and an auxiliary (a soft-max activation function to classify the labels) portion. The generator went through two layers of batch-normalization and up-sampling processed before 2-D convoluted networks were applied. A Tanh activation function was applied at the end to ensure the generated values fell between (-1, 1).</p> <h3 id="experiments">Experiments</h3> <p>Three assumptions on DS-1 (data distribution biased toward non-target), DS-2 (unbiased dataset) and DS-3 (data distribution biased toward target) mixture with 10%, 50% and 90% population set added as DS-2 were experimented. The three data mixtures were purposely selected to ensure that the overall mixture sizes were similar to one another and to avoid additional noise attributed from different size of the samples. The test sets were what we held out as unbiased sets from the individual datasets before selection biasness applied.</p> <p><strong>Experiment #1 (Data Mixture 1): 50% DS-1, 50% DS-2, 50% DS-3</strong></p> <p><strong>Experiment #2 (Data Mixture 2): 90% DS-1, 10% DS-2, 50% DS-3</strong></p> <p><strong>Experiment #3 (Data Mixture 3): 50% DS-1, 10% DS-2, 90% DS-3</strong></p> <h3 id="analysis">Analysis</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/table2_gan-480.webp 480w,/assets/img/table2_gan-800.webp 800w,/assets/img/table2_gan-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/table2_gan.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 2. Comparison of data mixture in biasness and model accuracy </div> <p><strong>Key-takeaway #1: A row-stacking data mixture would always benefit direct inference when the target label was known.</strong></p> <ul> <li>Quite intuitive as more data in general helps in informing less biased decision, as shown in <strong>Table 2</strong> when comparing values in the first set of comparisons.</li> <li>As observed in scenarios of experiment #2 and #3 (more likely to be the case of RWD where the directions of biasness are likely unknown and at different scale in sets of data), the benefit of aggregating the datasets was further highlighted.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/exp123456-480.webp 480w,/assets/img/exp123456-800.webp 800w,/assets/img/exp123456-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/exp123456.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Example dataset bias shown with different sample size and different assumed population variation </div> <p><strong>Key-takeaway #2: As the less biased dataset dominating the data mixture, we made less biased data inference.</strong></p> <ul> <li>As observed in <strong>Figure 2</strong>, when the overall data size is fixed, the blue and orange trends tend to be closer to zero as the size of data mixture 2 became larger (green background expanded).</li> </ul> <p><strong>Key-takeaway #3: Model performance among different data mixtures are all within a range of 85 - 87% accuracy. However, we observed that the model performance increased as the size of the unbiased data mixture - 2 increased. This might indicate that the increase in unbiased dataset in model training avoided the model from overfitting.</strong></p> <p><strong>Key-takeaway #4: We encountered common GAN model training issue that the discriminator for label classification became too strong and the generator gave-up on improving its data generation to fool discriminator.</strong></p> <ul> <li>Therefore, the model accuracies reported in <strong>Table 2</strong> could only been considered as results of the supervised learning models classifying the labels from the training datasets at this point (rather than that we’re able to synthesize “less biased” data from the generator for better inference.)</li> </ul> <h3 id="conclusions">Conclusions</h3> <p>Findings of this study further confirmed our intuition that the quality and volume aggregation of data sources might be the most crucial parts in industry (e.g. health care) that are highly relied on real-world data sources. Across all simulated data biasness scenarios, we found that centralizing all datasets would almost always led to less data attributed biasness compared to individual largest datasets. Though the author failed to prove that involving a GAN framework as a more powerful in reducing overall data + model biasness, we explored and learned a lot of GAN concepts during this process. One immediate next step could have been to do more experiments in tuning the SGAN model to improve the generator performance. Beyond this, exploring GAN framework trained on federated data system (e.g. datasets sit on different vendors’ server) remains an interesting area that author would love to explore next. Last but not least, the original proposed framework was hoping to scope a dynamic reinforcement learning framework that incorporates the achievements here. As RWD datasets are commonly refreshed periodically, we expect including a multi-arm bandit like reinforcement learning component could help learning the data drifts more in time and allocate the data weights based on target outcome of interest, where the rewards being levels of biasness reduction from a bench-marking population statistics.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/rwd_gan.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Chi-Hsuan Chang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>