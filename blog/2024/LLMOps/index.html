<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LangSmith vs. Langfuse | Chi-Hsuan Chang </title> <meta name="author" content="Chi-Hsuan Chang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://achchg.github.io/blog/2024/LLMOps/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Chi-Hsuan</span> Chang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">LangSmith vs. Langfuse</h1> <p class="post-meta"> March 19, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/learn"> <i class="fa-solid fa-hashtag fa-sm"></i> learn</a>     ·   <a href="/blog/category/llm"> <i class="fa-solid fa-tag fa-sm"></i> llm</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>One of the questions I had when first started working with LLMs was around local development to support quick prototyping without worrying much of the cost (e.g. I’m still protytping for the GenAI usecase, I’d need to test the engineering pipeline multiple times which might lead to multiple prompt requests == “$$”.) or data privacy (e.g. What if I do not feel comfortable sharing my datasets, chat histories through OpenAI or Huggingface API calls to connect to remotely hosted LLMs?)</p> <p>In particular, I have experienced trying out <a href="https://gpt4all.io/index.html" rel="external nofollow noopener" target="_blank">GPT4ALL</a> and <a href="https://ollama.com" rel="external nofollow noopener" target="_blank">Ollama</a>. Here are some documentations:</p> <h4 id="gpt4all-github">GPT4ALL (<a href="https://github.com/nomic-ai/gpt4all" rel="external nofollow noopener" target="_blank">Github</a>)</h4> <ul> <li> <p>This was the very first approach I tried. It was easy to setup, and it also provides a chat client (front end component) available for downloads. I only tried the backend API by installing gpt4all and langchain. After that, we can download and try different LLMs model on the GPT4ALL leaderboard.</p> </li> <li>We can directly run GPT4 model with python using gpt4all (<a href="https://docs.gpt4all.io" rel="external nofollow noopener" target="_blank">documentation</a>) or using Langchain (<a href="https://python.langchain.com/docs/integrations/llms/gpt4all" rel="external nofollow noopener" target="_blank">documentation</a>): <pre><code class="language-{python}">from langchain_community.llms import GPT4All
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

local_path = 'path to where you save the model bin file'
llm = GPT4All(model=local_path, callbacks=[StreamingStdOutCallbackHandler()])

template = """{question}"""

prompt = PromptTemplate.from_template(template)
llm_chain = LLMChain(prompt=prompt, llm=llm)
question = "What would be a good time to eat lunch?"

llm_chain.run(question)
</code></pre> </li> <li>2024-03-18 edits: The above code would throw a pydantic validation error (<a href="https://github.com/langchain-ai/langchain/issues/7778" rel="external nofollow noopener" target="_blank">similar issues raise here</a>). With a few research online, this might be due to compatibiilty issue. The above code used to work on <code class="language-plaintext highlighter-rouge">pydantic==1.10.0</code>, <code class="language-plaintext highlighter-rouge">langchain==0.0.320</code> and <code class="language-plaintext highlighter-rouge">gpt4all==2.0.0</code>. However, now my environmet is using <code class="language-plaintext highlighter-rouge">pydantic=2.6.4</code> and <code class="language-plaintext highlighter-rouge">langchain==0.1.12</code>. <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>pydantic.v1.error_wrappers.ValidationError: 1 validation error for GPT4All __root__ -&gt; __root__
Serializable.__init__() takes 1 positional argument but 2 were given (type=type_erro
</code></pre></div> </div> <p>Therefore, it seems further experiment and research on pacakge versions is needed before using gpt4all.</p> </li> </ul> <h4 id="ollama-github">Ollama (<a href="https://github.com/ollama/ollama" rel="external nofollow noopener" target="_blank">Github</a>)</h4> <ul> <li> <p>Very easy to setup. I followed the quickstart steps to install ollama on my Mac, and then run ollama docker image to start my local container. After these steps, I’m able to pull or run different LLMs available on their leaderboard.</p> </li> <li>We can directly run Ollama models from Terminal after setup: <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>ollama run llama2 "What would be a good time to eat lunch?"
</code></pre></div> </div> </li> <li>If need to further develop with python, it is also well integrated with Langchain: <pre><code class="language-{python}">from langchain_community.llms import Ollama
from langchain_community.chat_models import ChatOllama
from langchain_core.messages import HumanMessage

llm = Ollama(model="llama2")
chat_model = ChatOllama()

text = "What would be a good time to eat lunch?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
</code></pre> </li> <li>If would like to use Ollama with a front-end interface, there are other github repos that have already implemented integration that we can leverage. I’ll document my findings in another journal!</li> </ul> <p>In general, I found the Ollama setup more easy to kick-off and to start my own experiments locally.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Chi-Hsuan Chang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>