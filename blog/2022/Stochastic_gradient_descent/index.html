<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Stochastic gradient descent | Chi-Hsuan Chang </title> <meta name="author" content="Chi-Hsuan Chang"> <meta name="description" content="Day 7 - 8"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/chang-folio/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/chang-folio/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/chang-folio/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/chang-folio/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://achchg.github.io/chang-folio/blog/2022/Stochastic_gradient_descent/"> <link defer rel="stylesheet" href="/chang-folio/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/chang-folio/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/chang-folio/"> <span class="font-weight-bold">Chi-Hsuan</span> Chang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/chang-folio/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/chang-folio/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/chang-folio/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/chang-folio/projects/">projects </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Stochastic gradient descent</h1> <p class="post-meta"> September 29, 2022 </p> <p class="post-tags"> <a href="/chang-folio/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/chang-folio/blog/tag/review"> <i class="fa-solid fa-hashtag fa-sm"></i> review</a>     ·   <a href="/chang-folio/blog/category/probability"> <i class="fa-solid fa-tag fa-sm"></i> probability</a>   <a href="/chang-folio/blog/category/gradient"> <i class="fa-solid fa-tag fa-sm"></i> gradient</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>I’m reviewing the gradient descent and stochastic gradient descent again with NLP in deep learning, which are common machine learning parameter optimization methods well covered by packages these days and we normally use them hand-waving. Here, I’ll use logistic regression as my example:</p> <h4 id="stochastic-gradient-descent">(Stochastic) Gradient descent</h4> <p>Our often goal to develop a ML model is to make better prediction (i.e. minimize the cost of a model, \(J(\theta)\)), where \(\theta\) is a single parameter of our model here. In gradient descent, we’d:</p> <ol> <li>randomly initiate a value for \(\theta\) at start</li> <li>use the whole sample space (or a batch of data) to calculate the cost of a model under the \(\theta\) estimation at iteration \(j\)</li> <li> <p>make the \(\theta\) estimation at the next iteration with the following algorithm (where \(\alpha\) is the learning rate): \(\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta)\)</p> </li> <li>repeat #3 until \(\theta_j\) is converged (\(J(\theta)\) is minimized)</li> </ol> <p>In stochastic gradient descent, instead, we’d repeat #1 - #3 at every single sample to approximate the gradient with single data point. This allows us to update the gradient and cost incrementally fairly quickly.</p> <h4 id="cost-of-logistic-regression">Cost of logistic regression</h4> <p>If considering a logistic regression model with the simpliest setup to classify a binary outcome \(y\) that follows a bernouli trial:</p> \[\begin{align*} y_i &amp; = \begin{cases} 1 &amp; \text{, p}\\ 0 &amp; \text{, 1-p} \end{cases} \\ y_i &amp; \sim \text{Bernouli}(p)\\ \\ f(y) &amp; = p^y(1-p)^{1-y}\\ \\ L(y) &amp; = \Pi_{i=1}^n p^y_i(1-p)^{1-y_i} \end{align*}\] \[\begin{align*} \text{logit}(p) &amp; = \alpha + \beta x_i \\ p_i &amp; = \frac{\exp(\alpha + \beta x_i)}{1 + \exp(\alpha + \beta x_i)} \end{align*}\] <p>Therefore, we can derive the log-likelihood function:</p> \[\begin{align*} \ell(y) &amp; = \Sigma_{i=1}^n y_i\log(p_i) + (1-y_i)\log(1-p_i)\\ &amp; = \Sigma_{i=1}^n \log(1-p_i) + y_i\log(\frac{p_i}{1-p_i})\\ &amp; = \Sigma_{i=1}^n -\log(1 + \exp(\alpha + \beta x_i)) + y_i(\alpha + \beta x_i)\\ \end{align*}\] <p>If we use the negative average log-likelihood (cross entropy) as the cost function (J) would be:</p> \[J = -\frac{1}{n}\Sigma_{i=1}^n y_i(\alpha + \beta x_i) + \log(1 + \exp(\alpha + \beta x_i))\] <p>And the gradients of \(\alpha\) and \(\beta\) are:</p> \[\begin{align*} \frac{\partial}{\partial \alpha} &amp; = -\frac{1}{n}\Sigma_{i=1}^n y_i +\frac{\exp(\alpha + \beta x_i)}{1 + \exp(\alpha + \beta x_i)} = \frac{1}{n}\Sigma_{i=1}^n(p_i-y_i) = \frac{1}{n}\Sigma_{i=1}^n(\hat{y_i}-y_i)\\ \frac{\partial}{\partial \beta} &amp; = -\frac{1}{n}\Sigma_{i=1}^n y_ix_i +\frac{\exp(\alpha + \beta x_i) x_i}{1 + \exp(\alpha + \beta x_i)}= \frac{1}{n}\Sigma_{i=1}^n x_i(p_i-y_i) = \frac{1}{n}\Sigma_{i=1}^n x_i(\hat{y_i}-y_i) \end{align*}\] <p>With gradient descent, we’d optimize \(\alpha\) and \(\beta\) with the following algorithm and learning rates:</p> \[\begin{align*} \alpha_j &amp; := \alpha_j - \eta_1 \frac{1}{n}\Sigma_{i=1}^n(\hat{y_i}-y_i)\\ \beta_j &amp; := \beta_j - \eta_2 \frac{1}{n}\Sigma_{i=1}^nx_i(\hat{y_i}-y_i) \end{align*}\] <p>Pseudo code that I wrote from CS224 assignment. Here x is the parameter to be optimized and step is the learning rate:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td> <td class="code"><pre> <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">start_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">step</span> <span class="o">*</span> <span class="n">grad</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Example notebook with above example can be found <a href="https://github.com/achchg/achchg.github.io/blob/master/jupyternb/2022-09-29-Stochastic_gradient_descent.ipynb" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>Great reference material can be found <a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf" rel="external nofollow noopener" target="_blank">here</a></p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Chi-Hsuan Chang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/chang-folio/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/chang-folio/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/chang-folio/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/chang-folio/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/chang-folio/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/chang-folio/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>