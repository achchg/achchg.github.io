---
layout: post
title: RNN/LSTM
date: 2022-10-07
description: Day 15
tags: review
categories: dl sequence
---

My notes from reviewing RNN model in NLP, following the flow of [RNN](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf) and [Seq2seq](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf) lecture notes.

#### RNN architecture

I'll use the below figure cited from CS224n lecture 5-6 notes of RNN to summarize RNN:

"The elements (e.g. words) were fed into the algorithm one after one along (e.g. the $$t^{th}$$ element of the input sequence, $$w_{t}$$) with the hidden output layer ($$h_{t-1}$$) from the previous timestamp ($$t-1$$) in predicting the most likely next element of the output sequence ($$y_{t}$$)" 

$$
\begin{align*}
h_t & = \sigma(W^{(hh)}h_{t−1} + W^{(hx)}x_{t})\\
y_t & = \text{softmax}(W^{(S)}h_t)
\end{align*}
$$


<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/rnn.png" title="example image" %}
    </div>
</div>

- Parameters to be solved: $$h_t$$, $$W^{(hh)}$$, $$W^{(hx)}$$ and $$W^{(S)}$$
- With the softmax activation function, the common loss function for RNN is the cross-entropy loss (as derived previously in the [SGD post with logistic regression](https://achchg.github.io/blog/2022/Stochastic_gradient_descent/)):


$$
\begin{align*}
J^{(t)}(\theta) &= −\Sigma_{j=1}^{|V|}y_{t,j} * log(\hat{y}_{t,j}) \\
J(\theta) &= \frac{1}{T}\Sigma_{t=1}^TJ^{(t)}(\theta)
\end{align*}
$$

- As the sequence is getting longer, there could be vanishing/exploding gradient problem. As we'd solve for the gradient for $$J^{(t)}(\theta)$$ with chain rule with respect to multiple layers of t. The gradient can become very small or large. One way to solve the exploding gradient problem is to setup a threshold for the gradient, as the gradient exceeds the threshold, adjust the gradient to a smaller level.
- Evaluation method of RNN model: **Perplexity!** The lower the perplexity, the better the model.

$$
\begin{align*}
\text{perplexity} = \exp({J})
\end{align*}
$$


#### Long Short Term Memory (LSTM)
Here is a very nicely written [blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) and where I referenced the below figure, but I'd take my own notes below:

"LSTM is a special form of RNN that aimed to avoid the long-term dependency problem due to long sequence." 


$$
\begin{align*}
\color{Cerulean}\text{forget gate: } & \color{Cerulean}\boxed{f^{(t)}  = \sigma(W_fh_{t−1} + U_fx_{t} + b_f)}\\
&\color{Cerulean}\textnormal{what content from t-1 is kept, $f^{(t)}$ is between 0 and 1; larger means "memory keeping"}\\
\color{blue}\text{input gate: } & \color{blue}\boxed{i^{(t)} = \sigma(W_fi_{t−1} + U_ix_{t} + b_i)}\\
\color{darkblue}\text{output gate: } & \color{darkblue}\boxed{o^{(t)} = \sigma(W_fo_{t−1} + U_ox_{t} + b_0)}\\
\\
\color{Lavender} \text{new cell content: } & \color{Lavender}\boxed{\tilde{c}_{t} = \tanh(W_ch_{t-1} + U_cx_{t} + b_c)}\\
\color{Purple} \text{new cell state: } & \color{Purple}\boxed{c_{t} = f_{t} \cdot c_{t-1} + i_t \cdot \tilde{c}_{t}} \\
&\color{Purple}\textnormal{new and carryover contents to be written}\\
\\
& h_t = o_t \cdot \tanh c_t\\
&\textnormal{new memory to be ouput}\\
\end{align*}
$$


<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/lstm.png" title="lstm image" %}
    </div>
</div>


Nice RNN resources:
- [CS224N lecture 5](https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture05-rnnlm.pdf)
- [CS224N lecture 6](https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture06-fancy-rnn.pdf)
- [StatQuest](https://www.youtube.com/watch?v=AsNTP8Kwu80)
  