---
layout: post
title: How to run LLMs locally on your machine?
date: 2024-03-12
description: 
tags: learn
categories: llm
---
One of the questions I had when first started working with LLMs was around local development to support quick prototyping without worrying much of the cost (e.g. I'm still protytping for the GenAI usecase, I'd need to test the engineering pipeline multiple times which might lead to multiple prompt requests == "$$".) or data privacy (e.g. What if I do not feel comfortable sharing my datasets, chat histories through OpenAI or Huggingface API calls to connect to remotely hosted LLMs?)

In particular, I have experienced trying out [GPT4ALL](https://gpt4all.io/index.html) and [Ollama](https://ollama.com). Here are some documentations:


#### GPT4ALL ([Github](https://github.com/nomic-ai/gpt4all))
- This was the very first approach I tried. It was easy to setup, and it also provides a chat client available for downloads. I only tried the backend API by installing gpt4all and langchain. After that, we can download and try different LLMs model on the GPT4ALL leaderboard.

- We can directly run GPT4 model with python using gpt4all ([documentation](https://docs.gpt4all.io)) or using Langchain ([documentation](https://python.langchain.com/docs/integrations/llms/gpt4all.html)):
  ```{python}
  from langchain_community.llms import GPT4All
  from langchain.chains import LLMChain
  from langchain.prompts import PromptTemplate
  from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

  local_path = 'path to where you save the model bin file'
  llm = GPT4All(model=local_path, callbacks=[StreamingStdOutCallbackHandler()])

  template = """{question}"""

  prompt = PromptTemplate.from_template(template)
  llm_chain = LLMChain(prompt=prompt, llm=llm)
  question = "What would be a good time to eat lunch?"

  llm_chain.run(question)
  ````


#### Ollama ([Github](https://github.com/ollama/ollama))
- Very easy to setup. I followed the quickstart steps to install ollama on my Mac, and then run ollama docker image to start my local container. After these steps, I'm able to pull or run different LLMs available on their leaderboard.

- We can directly run Ollama models from Terminal after setup:
  ```
  ollama run llama2 "What would be a good time to eat lunch?"
  ```

- If need to further develop with python, it is also well integrated with Langchain:
  ```{python}
  from langchain_community.llms import Ollama
  from langchain_community.chat_models import ChatOllama
  from langchain_core.messages import HumanMessage

  llm = Ollama(model="llama2")
  chat_model = ChatOllama()

  text = "What would be a good time to eat lunch?"
  messages = [HumanMessage(content=text)]

  llm.invoke(text)
  ````



